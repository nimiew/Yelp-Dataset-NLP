{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AJL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AJL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST CHANGE ACCORDINGLY!\n",
    "GLOVE_LOCATION = r'D:\\Downloads\\glove.twitter.27B\\glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.split()\n",
    "    avg = np.zeros(200) # depends on glove\n",
    "    num_valid_words = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            num_valid_words += 1\n",
    "    if num_valid_words == 0:\n",
    "        return np.zeros(200)\n",
    "    else:\n",
    "        return avg / num_valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15300/15300 [00:14<00:00, 1078.45it/s]\n",
      "100%|██████████| 15300/15300 [00:01<00:00, 10241.91it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\") \n",
    "cleaned_reviews = []\n",
    "reviews = df['text']\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "for review in tqdm(reviews):\n",
    "    sentences = nltk.tokenize.sent_tokenize(review)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        for token in tokenizer.tokenize(sentence):\n",
    "            token = token.lower()\n",
    "            if token.isalpha() and not token in stopwords:\n",
    "                tokens.append(token)\n",
    "    joined = \" \".join(tokens)\n",
    "    joined = joined.replace('\\n', ' ').replace('\\r', '')\n",
    "    cleaned_reviews.append(joined)\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(GLOVE_LOCATION)\n",
    "X = np.array([sentence_to_avg(x, word_to_vec_map) for x in tqdm(cleaned_reviews)])\n",
    "X = np.nan_to_num(X)\n",
    "Y = np.array(df['stars']).astype('int') - 1 # change to 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_tfid = vectorizer.fit_transform(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X.npy', X)\n",
    "np.save('X_tfid.npy', X_tfid)\n",
    "np.save('Y.npy', Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
